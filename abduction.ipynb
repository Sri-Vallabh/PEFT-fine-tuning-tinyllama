{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52518343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "043d0be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b520b35085d4b48b15270b8816d864d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/133799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of abhidharma dataset: {'category': 'abhidharma', 'question': 'What does the term Abhidharma mean in the Buddhist exegetical tradition?', 'answer': 'In the Buddhist exegetical tradition, the term Abhidharma means approximately “higher” or “further” teaching, and it refers both to the doctrinal investigations of the new scholastic movement and to the body of texts yielded by its systematic exposition of Buddhist thought. This body of literature includes the third of the “three baskets” (Skt., tripiṭaka, Pali, tipiṭaka) of the Buddhist canon, namely, the Abhidharma-piṭaka (Pali, Abhidhamma-piṭaka), its commentaries, and later exegetical texts.'}\n",
      "Last row of abhidharma dataset: {'category': 'abhidharma', 'question': 'What is a school of thought in Japanese Philosophy that is heavily influenced by Western Philosophy?', 'answer': 'The Kyoto School is a school of thought in Japanese Philosophy that is heavily influenced by Western Philosophy.'}\n",
      "Length of abhidharma dataset: 54\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load the original dataset\n",
    "orig_dataset = load_dataset(\"json\", data_files=\"philosophy_qa_fixed.json\")[\"train\"]\n",
    "\n",
    "# Filter samples with category == \"abduction\"\n",
    "abduction_samples = orig_dataset.filter(lambda x: x[\"category\"] == \"abhidharma\")\n",
    "\n",
    "# Create a new dataset from the filtered samples\n",
    "dataset = Dataset.from_list(abduction_samples)\n",
    "\n",
    "# Print for verification\n",
    "print('First row of abhidharma dataset:', dataset[0])\n",
    "print('Last row of abhidharma dataset:', dataset[-1])\n",
    "print('Length of abhidharma dataset:', len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50f347ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import os\n",
    "\n",
    "class SaveBestAndEpochCallback(TrainerCallback):\n",
    "    def __init__(self, trainer, adapter_dir=\"./tinyllama--adapter-repeated\"):\n",
    "        super().__init__()\n",
    "        self.trainer = trainer\n",
    "        self.adapter_dir = adapter_dir\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.final_dir = os.path.join(os.path.dirname(adapter_dir), \"final_abhidharma_adapter\")\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            current_loss = logs[\"loss\"]\n",
    "            if current_loss < self.best_loss:\n",
    "                self.best_loss = current_loss\n",
    "                self.trainer.model.save_pretrained(self.adapter_dir)\n",
    "                print(f\"Step {state.global_step}: New best loss = {current_loss:.3f} → Model saved.\")\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        # Save the final adapter\n",
    "        os.makedirs(self.final_dir, exist_ok=True)\n",
    "        self.trainer.model.save_pretrained(self.final_dir)\n",
    "        print(f\"Training complete! Final adapter saved to: {self.final_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "617b0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a philosophical AI assistant. Answer the question.<|im_end|>\n",
    "<|im_start|>user\n",
    "{sample['question']}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{sample['answer']}<|im_end|>\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e23622a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52873db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": \"cuda:0\"},\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86b95189",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = prepare_model_for_kbit_training(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22f3ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # TinyLlama attention layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80cebf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "# Apply LoRA to the base model\n",
    "model = get_peft_model(base_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9dca6c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1126400\n"
     ]
    }
   ],
   "source": [
    "print(f\"Trainable parameters: {sum(p.numel() for p in base_model.parameters() if p.requires_grad)}\")\n",
    "# Should output ~8.4M for r=8, ~16.8M for r=16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cc99397",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-abhidharma-adapter\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=5e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=1,  # Critical: log loss at every step\n",
    "    num_train_epochs=24,\n",
    "    fp16=False,\n",
    "     max_grad_norm=1.0, \n",
    "    save_strategy=\"no\",  # Disable default checkpoint saving\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49612125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c8ddb78ecd461fb4401f72bf9f175e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8031d4c64d7745669b1222ef87bc55d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc3c370844b4f1d87bafee971c2a15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82e3c091ef4430e8956da5a409cb258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b21ce9bd34f4e6cac8495e507760c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    # peft_config=peft_config,\n",
    "    formatting_func=format_instruction,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Add the callback with the trainer instance\n",
    "trainer.add_callback(SaveBestAndEpochCallback(trainer))  # Pass the trainer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99c18991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [168/168 11:36, Epoch 24/24]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.368600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.755900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.873400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.773700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.659500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.514400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.295100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.406700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.376900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.265600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.301200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.201400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.357700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.226200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.203300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.183600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.999300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.755100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.889800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.896500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.772700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.775300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.769100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.749200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.735200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.672400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.679300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.599700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.559900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.642300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.719500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.610500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.561300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.566300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.562200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.527100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.565300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.538100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.474300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.457600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.421200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.465200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.389800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.365900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.475300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.301200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.361800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.273500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.309300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.308100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.287600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.267700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.189400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.226200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.214600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.176700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.202200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.240500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.163600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.161200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.150900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.157700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.166300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.156800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: New best loss = 2.384 → Model saved.\n",
      "Step 3: New best loss = 2.369 → Model saved.\n",
      "Step 4: New best loss = 2.352 → Model saved.\n",
      "Step 5: New best loss = 2.206 → Model saved.\n",
      "Step 6: New best loss = 2.162 → Model saved.\n",
      "Step 7: New best loss = 2.142 → Model saved.\n",
      "Step 8: New best loss = 2.108 → Model saved.\n",
      "Step 9: New best loss = 2.102 → Model saved.\n",
      "Step 10: New best loss = 1.756 → Model saved.\n",
      "Step 15: New best loss = 1.659 → Model saved.\n",
      "Step 16: New best loss = 1.477 → Model saved.\n",
      "Step 19: New best loss = 1.295 → Model saved.\n",
      "Step 22: New best loss = 1.266 → Model saved.\n",
      "Step 26: New best loss = 1.201 → Model saved.\n",
      "Step 28: New best loss = 1.059 → Model saved.\n",
      "Step 37: New best loss = 1.021 → Model saved.\n",
      "Step 39: New best loss = 0.927 → Model saved.\n",
      "Step 47: New best loss = 0.755 → Model saved.\n",
      "Step 55: New best loss = 0.747 → Model saved.\n",
      "Step 57: New best loss = 0.723 → Model saved.\n",
      "Step 63: New best loss = 0.672 → Model saved.\n",
      "Step 65: New best loss = 0.600 → Model saved.\n",
      "Step 66: New best loss = 0.560 → Model saved.\n",
      "Step 79: New best loss = 0.523 → Model saved.\n",
      "Step 80: New best loss = 0.404 → Model saved.\n",
      "Step 85: New best loss = 0.385 → Model saved.\n",
      "Step 94: New best loss = 0.366 → Model saved.\n",
      "Step 98: New best loss = 0.332 → Model saved.\n",
      "Step 100: New best loss = 0.301 → Model saved.\n",
      "Step 106: New best loss = 0.274 → Model saved.\n",
      "Step 110: New best loss = 0.267 → Model saved.\n",
      "Step 113: New best loss = 0.237 → Model saved.\n",
      "Step 116: New best loss = 0.177 → Model saved.\n",
      "Step 127: New best loss = 0.156 → Model saved.\n",
      "Step 133: New best loss = 0.155 → Model saved.\n",
      "Step 136: New best loss = 0.118 → Model saved.\n",
      "Training complete! Final adapter saved to: .\\final_abhidharma_adapter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=168, training_loss=0.6931155103125742, metrics={'train_runtime': 709.6398, 'train_samples_per_second': 1.826, 'train_steps_per_second': 0.237, 'total_flos': 1969486703198208.0, 'train_loss': 0.6931155103125742})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# # 8. Save adapter\n",
    "# trainer.model.save_pretrained(\"./tinyllama-philosophy-adapter-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8a12444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Assuming your model is called 'model' and is on GPU\n",
    "model.to('cpu')     # Move model to CPU\n",
    "del model           # Delete the model object\n",
    "gc.collect()        # Run garbage collection\n",
    "torch.cuda.empty_cache()  # Empty PyTorch's CUDA cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51291d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from fine-tuned model:\n",
      "<|im_start|>system\n",
      "You are a philosophical AI assistant. Answer questions using wisdom from great philosophers.<|im_end|>\n",
      "<|im_start|>user\n",
      "Who did Abelard have a conflict with because he believed reasoning had a role to play in matters of faith?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Abelard had a conflict with those who were advocates of the individual church for the sake of the church’s rule, such as the rule followed in matters of faith, because he believed reasoning had a role to play in matters of faith and would object that those who adhered to individual churches were no less faithful to the will of God than those who were subject to the authority of the Roman Catholic Church.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (if not already installed)\n",
    "# !pip install transformers peft torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and base model\n",
    "base_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "adapter_path = \"./final_abelard_adapter\"  # Path to the saved fine-tuned adapter\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Load the fine-tuned adapter on top of the base model\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model = model.merge_and_unload()\n",
    "model.eval()\n",
    "\n",
    "# Prepare a prompt with a placeholder question\n",
    "question = \"Who did Abelard have a conflict with because he believed reasoning had a role to play in matters of faith?\"  # <-- Replace with your question\n",
    "prompt = f\"\"\"<|im_start|>system\n",
    "You are a philosophical AI assistant. Answer questions using wisdom from great philosophers.<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "# Generate response from the fine-tuned model\n",
    "model_device = next(model.parameters()).device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.3,  # More deterministic\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the response\n",
    "print(\"Response from fine-tuned model:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7f861b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from fine-tuned model:\n",
      "<|im_start|>system\n",
      "You are a philosophical AI assistant. Answer questions using wisdom from great philosophers.<|im_end|>\n",
      "<|im_start|>user\n",
      "Who did Abelard have a conflict with because he believed reasoning had a role to play in matters of faith?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Abelard had a conflict with those who advocated a rationalism that undermined the faith, including his own university, because he believed reasoning had a role to play in matters of faith and would not hesitate to criticize those who took a similar position.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Prepare a prompt with a placeholder question\n",
    "question = \"Who did Abelard have a conflict with because he believed reasoning had a role to play in matters of faith?\"  # <-- Replace with your question\n",
    "prompt = f\"\"\"<|im_start|>system\n",
    "You are a philosophical AI assistant. Answer questions using wisdom from great philosophers.<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "# Generate response from the fine-tuned model\n",
    "model_device = next(model.parameters()).device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.1,  # More deterministic\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the response\n",
    "print(\"Response from fine-tuned model:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42694ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4106f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
